# PointPillars(2018, CVPR 2019)

[GitHub - zhulf0804/PointPillars: A Simple PointPillars PyTorch Implenmentation for 3D Lidar(KITTI) Detection.](https://github.com/zhulf0804/PointPillars)

## Characteristic(Contributions)

- Point cloud data를 vertical 방향으로 인코딩하여 3D convolution이 아닌 2D convolution 연산 적용이 가능해짐(빠른 연산이 가능해짐)
- x, y, z로 나누어 grouping 했던 voxel과 달리 x, y로만 나누고, z 축에는 한계가 없도록 grouping함, 이때 z 축의 정보가 사라질 수 있기 때문에 pilar 내부의 point 들의 평균 중심점을 포함하여 z축 정보를 함께 인코딩함
- Fixed encoder를 이용하는게 아닌 feature를 학습해서 모든 point cloud 정보를 활용할 수 있음

## Model Structure

![Untitled](PointPillars(2018,%20CVPR%202019)%20646e2a707bfc498fb0856a4e3ca40682/Untitled.png)

모델의 전체적인 구성은 Pointcloud to Pseudo-Image, Backbone, Detection head의 세가지로 구분된다. Raw point cloud data를 pillar형태로 변환하여 각 pillar들을 stack하고, index와 함께 저장한다. 이렇게 변환된 내용들을 토대로 인코더는 2D pseudo image로 변환하는데 사용되는 feature를 학습한다. 

## Loss Function

![Untitled](PointPillars(2018,%20CVPR%202019)%20646e2a707bfc498fb0856a4e3ca40682/Untitled%201.png)

![Untitled](PointPillars(2018,%20CVPR%202019)%20646e2a707bfc498fb0856a4e3ca40682/Untitled%202.png)

SECOND에서와 동일한 loss function을 사용하고, ground truth와 anchor 정보는 x, y, z, w, h, l, theta로 구성된다.

Loss는 위와 같이 계산되며, angle localization 만으로는 뒤집어진 box를 구별하지 못하기 때문에 softmax classification을 이용하여 heading을 학습한다.

$P^a$는 anchor의 class probability이고, 해당 논문에서는 $\alpha$ = 0.25, $\gamma$ = 2로 설정했다.

![Untitled](PointPillars(2018,%20CVPR%202019)%20646e2a707bfc498fb0856a4e3ca40682/Untitled%203.png)

위의 식이 total loss이고, $N_{pos}$는 positive anchor의 수를 의미한다. 여기서 $B_{loc} = 2, B_{cls} = 1, B_{dir} = 0.2$ 로 설정하였다.

## Model Performance

### Inference accuracy

- For KITTI validation set

![Untitled](PointPillars(2018,%20CVPR%202019)%20646e2a707bfc498fb0856a4e3ca40682/Untitled%204.png)

### Inference speed

- For KITTI dataset with a 1080ti GPU(11.34 TOPS(FP32), Tensor Cores None, CUDA Cores 3584) : 16.2ms